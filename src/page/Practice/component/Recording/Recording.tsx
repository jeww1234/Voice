import "./Recording.style.css";
import { usePracticeStore } from "../../../../store/usePracticeStore";
import { useEffect, useRef, useState } from "react";
import { LipSyncAnalyzer } from "../../../../utils/LipSyncAnalyzer";

declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }

  interface SpeechRecognition extends EventTarget {
    continuous: boolean;
    interimResults: boolean;
    lang: string;
    start: () => void;
    stop: () => void;
    onresult: (event: SpeechRecognitionEvent) => void;
    onerror: (event: any) => void;
    onend: () => void;
  }

  interface SpeechRecognitionEvent extends Event {
    results: SpeechRecognitionResultList;
  }
}

const Recording = () => {
  const lipSyncAnalyzer = useRef(new LipSyncAnalyzer()).current;

  const {
    setAnalysisResult,
    speechResult,
    currentSentence,
    isRecording,
    setRecording,
    addChunk,
    recordedChunks,
    setSpeechResult,
    setRecordedChunks,
  } = usePracticeStore();

  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const recordBtnRef = useRef<HTMLButtonElement>(null);
  const stopBtnRef = useRef<HTMLButtonElement>(null);
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const [lipDetected, setLipDetected] = useState(false);
  const [lipLandmarksHistory, setLipLandmarksHistory] = useState<any[]>([]); // ì…ìˆ  ëœë“œë§ˆí¬ íˆìŠ¤í† ë¦¬ ìƒíƒœ ì •ì˜

  // ìŒì„± ì¸ì‹ ìƒíƒœ ì¶”ê°€
  const [isSpeechRecording, setIsSpeechRecording] = useState(false);

  const recognitionRef = useRef<SpeechRecognition | null>(null); // ìŒì„± ì¸ì‹ ê°ì²´ë¥¼ refë¡œ ì €ì¥

  // ì¹´ë©”ë¼ ì´ˆê¸°í™”
  const initializeCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480 },
        audio: true,
      });

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }
      streamRef.current = stream;
    } catch (error) {
      console.error("Camera access error:", error);
      alert("ì¹´ë©”ë¼ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.");
    }
  };

  // FaceMesh ì´ˆê¸°í™”
  const initializeFaceMesh = async () => {
    const videoElement = videoRef.current;
    const canvasElement = canvasRef.current;
    const canvasCtx = canvasElement?.getContext("2d");

    if (!videoElement || !canvasElement || !canvasCtx) return;

    canvasElement.width = 640;
    canvasElement.height = 480;

    const faceMesh = new (window as any).FaceMesh({
      locateFile: (file: string) =>
        `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });

    const captureLipData = (landmarks: any) => {
      const upperLip = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291];
      const lowerLip = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291];

      // ì…ìˆ  ì¢Œí‘œ ì¶”ì¶œ
      const lipCoords = [...upperLip, ...lowerLip].map((index) => {
        const landmark = landmarks[index]; // ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ëœë“œë§ˆí¬
        // landmarkê°€ ìœ íš¨í•œì§€ ì²´í¬í•˜ê³ , ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        if (
          landmark &&
          typeof landmark.x === "number" &&
          typeof landmark.y === "number"
        ) {
          return {
            x: landmark.x * 640, // í™”ë©´ ë„ˆë¹„ì— ë§ê²Œ ì¢Œí‘œ ë¹„ìœ¨ ì¡°ì •
            y: landmark.y * 480, // í™”ë©´ ë†’ì´ì— ë§ê²Œ ì¢Œí‘œ ë¹„ìœ¨ ì¡°ì •
          };
        } else {
          // ì˜ëª»ëœ ì¸ë±ìŠ¤ë‚˜ ëœë“œë§ˆí¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ (0, 0)ìœ¼ë¡œ ì²˜ë¦¬
          console.warn(`ì˜ëª»ëœ ëœë“œë§ˆí¬ ì¸ë±ìŠ¤: ${index} ë˜ëŠ” ê°’ì´ ë¹„ì–´ìˆìŒ`);
          return { x: 0, y: 0 };
        }
      });

      // lipLandmarksHistory ë°°ì—´ì— ì¢Œí‘œ ì¶”ê°€
      setLipLandmarksHistory((prevHistory) => [...prevHistory, lipCoords]);
    };

    //ì…ìˆ  ì¶”ì 
    faceMesh.onResults((results: any) => {
      // ìº”ë²„ìŠ¤ë¥¼ ì´ˆê¸°í™”
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

      if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
        const landmarks = results.multiFaceLandmarks[0]; // ì²« ë²ˆì§¸ ì–¼êµ´ì˜ ëœë“œë§ˆí¬

        // ì…ìˆ  ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        drawLipLandmarks(canvasCtx, landmarks);
        // ì…ìˆ  ì¢Œí‘œë¥¼ ì €ì¥ í•¨ìˆ˜ í˜¸ì¶œ
        captureLipData(landmarks);
        lipSyncAnalyzer.captureLipData(landmarks, "ko");

        setLipDetected(true); // ì…ìˆ ì´ ì¸ì‹ëœ ìƒíƒœë¡œ ì„¤ì •
      } else {
        setLipDetected(false); // ì…ìˆ ì´ ì¸ì‹ë˜ì§€ ì•Šìœ¼ë©´ false
      }
    });

    const extractLipLandmarks = (landmarks: any) => {
      // ìƒì…ìˆ , í•˜ì…ìˆ  ëœë“œë§ˆí¬ ì¸ë±ìŠ¤
      const upperLip = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]; // ìƒì…ìˆ  ì¸ë±ìŠ¤
      const lowerLip = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]; // í•˜ì…ìˆ  ì¸ë±ìŠ¤

      // ì…ìˆ  ëœë“œë§ˆí¬ ì¢Œí‘œ ì¶”ì¶œ
      const lipCoords = [...upperLip, ...lowerLip].map((index) => {
        return {
          x: landmarks[index].x * 640, // í™”ë©´ ë„ˆë¹„ì— ë§ê²Œ ë¹„ìœ¨ ì¡°ì •
          y: landmarks[index].y * 480, // í™”ë©´ ë†’ì´ì— ë§ê²Œ ë¹„ìœ¨ ì¡°ì •
        };
      });

      return lipCoords; // ì…ìˆ  ì¢Œí‘œ ë°˜í™˜
    };

    const camera = new (window as any).Camera(videoElement, {
      onFrame: async () => {
        await faceMesh.send({ image: videoElement });
      },
      width: 640,
      height: 480,
    });
    camera.start();
  };

  // ì…ìˆ  ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
  const drawLipLandmarks = (ctx: CanvasRenderingContext2D, landmarks: any) => {
    const upperLip = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]; // ìƒì…ìˆ  ì¸ë±ìŠ¤
    const lowerLip = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]; // í•˜ì…ìˆ  ì¸ë±ìŠ¤

    // ìƒì…ìˆ  ê·¸ë¦¬ê¸°
    ctx.strokeStyle = "red";
    ctx.lineWidth = 2;
    ctx.beginPath();
    upperLip.forEach((index, i) => {
      const x = landmarks[index].x * 640;
      const y = landmarks[index].y * 480;
      if (i === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
    });
    ctx.closePath();
    ctx.stroke();

    // í•˜ì…ìˆ  ê·¸ë¦¬ê¸°
    ctx.strokeStyle = "red";
    ctx.lineWidth = 2;
    ctx.beginPath();
    lowerLip.forEach((index, i) => {
      const x = landmarks[index].x * 640;
      const y = landmarks[index].y * 480;
      if (i === 0) {
        ctx.moveTo(x, y);
      } else {
        ctx.lineTo(x, y);
      }
    });
    ctx.closePath();
    ctx.stroke();
  };

  // ì¹´ë©”ë¼ & FaceMesh ì´ˆê¸°í™” useEffect
  useEffect(() => {
    initializeCamera();
    initializeFaceMesh();
  }, []);

  // ìŒì„± ì¸ì‹ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ)
  const initializeSpeechRecognition = () => {
    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (SpeechRecognition) {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true; // interimResultsë¥¼ trueë¡œ ì„¤ì •í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŒì„± ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.

      recognition.onresult = (event: any) => {
        const transcript =
          event.results[event.results.length - 1][0].transcript;
        setSpeechResult(transcript); // ìŒì„± ì¸ì‹ ê²°ê³¼ ì—…ë°ì´íŠ¸
      };

      recognition.onerror = (event: any) => {
        console.error("SpeechRecognition error", event.error);
      };

      recognition.onend = () => {
        setIsSpeechRecording(false); // ìŒì„± ì¸ì‹ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ìƒíƒœ ë³€ê²½
      };

      recognitionRef.current = recognition; // ìŒì„± ì¸ì‹ ê°ì²´ë¥¼ refë¡œ ì €ì¥
    } else {
      alert("Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.");
    }
  };
  
  
  const startRecording = () => {
    if (!streamRef.current || !recordBtnRef.current || !stopBtnRef.current)
      return;

    setSpeechResult("");
    setRecordedChunks([]);

    // ë²„íŠ¼ ìƒíƒœ
    recordBtnRef.current.disabled = true;
    stopBtnRef.current.disabled = false;
    recordBtnRef.current.classList.add("recording");

    setRecording(true);

    // ğŸ¥ ì˜ìƒ ë…¹í™” ì‹œì‘
    const recorder = new MediaRecorder(streamRef.current);

    recorder.ondataavailable = (event) => {
      if (event.data.size > 0) addChunk(event.data);
    };

    recorder.start();
    mediaRecorderRef.current = recorder;

    // ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘
    startSpeechRecognition();
  };

  const stopRecording = () => {
    console.log("stop");
    if (!recordBtnRef.current || !stopBtnRef.current) return;

    // ë²„íŠ¼ ìƒíƒœ
    recordBtnRef.current.classList.remove("recording");
    recordBtnRef.current.disabled = false;
    stopBtnRef.current.disabled = true;

    setRecording(false);

    if (
      mediaRecorderRef.current &&
      mediaRecorderRef.current.state !== "inactive"
    ) {
      mediaRecorderRef.current.onstop = async () => {
        if (speechResult && speechResult.trim().length > 0) {
          const result = await lipSyncAnalyzer.analyzeLipSync(
            currentSentence,
            speechResult,
            "ko"
          );
          console.log("ìµœì¢… ì ìˆ˜:", result.finalScore);
          console.log("ìƒì„¸ ë¶„ì„:", result.detailedAnalysis);
          setAnalysisResult(result);
        } else {
          console.warn("âš ï¸ ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ. ë¶„ì„ ìƒëµë¨.");
        }
      };
      mediaRecorderRef.current.stop();
    }
    stopSpeechRecognition();
  };

  // ìŒì„± ì¸ì‹ ì‹œì‘
  const startSpeechRecognition = () => {
    if (!recognitionRef.current) {
      initializeSpeechRecognition();
    }
    recognitionRef.current?.start(); // ìŒì„± ì¸ì‹ ì‹œì‘
    setIsSpeechRecording(true);
  };

  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopSpeechRecognition = () => {
    recognitionRef.current?.stop(); // ìŒì„± ì¸ì‹ ì¤‘ì§€
  };

  const blob = new Blob(recordedChunks, { type: "video/webm" });
  const url = URL.createObjectURL(blob);
  return (
    <div className="">
      <div className="practice-header">
        <h2>ë°œìŒ ì—°ìŠµ</h2>
      </div>

      <div className="sentence-display">
        <h3>
          ì—°ìŠµí•  ë¬¸ì¥ : <span id="targetSentence">{currentSentence}</span>
        </h3>
      </div>
      <div className="video-container">
        <video id="videoElement" ref={videoRef} autoPlay playsInline></video>
        <canvas id="lipCanvas" ref={canvasRef}></canvas>
        <div
          id="lipIndicator"
          className="lip-indicator"
          style={{
            backgroundColor: lipDetected
              ? "rgba(46, 204, 113, 0.9)" // ì´ˆë¡ìƒ‰ (ì…ìˆ  ì¸ì‹ë¨)
              : "rgba(102, 126, 234, 0.9)", // íŒŒë€ìƒ‰ (ì¸ì‹ ì¤‘)
          }}
        >
          {lipDetected ? "âœ“ ì…ìˆ  ì¸ì‹ë¨" : "ì…ìˆ  ì¸ì‹ ì¤‘..."}
        </div>
      </div>

      <div className="control-buttons">
        <button
          className="btn-control"
          id="recordBtn"
          ref={recordBtnRef}
          onClick={startRecording}
        >
          <span className="icon">
            ğŸ™ï¸<span>ë…¹í™” ì‹œì‘</span>
          </span>
        </button>
        <button
          className="btn-control"
          id="stopBtn"
          ref={stopBtnRef}
          onClick={stopRecording}
        >
          <span className="icon">
            ğŸŸ¥<span>ë…¹í™” ì¤‘ì§€</span>
          </span>
        </button>
        <label className="btn-control" htmlFor="fileInput">
          <span className="icon">
            ğŸ“<span>íŒŒì¼ ì—…ë¡œë“œ</span>
          </span>
        </label>
        <input
          type="file"
          id="fileInput"
          accept="video/*"
          style={{ display: "none" }}
        />
      </div>
    </div>
  );
};

export default Recording;
